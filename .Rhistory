java)
#tfidf <- clean_allcsv1
tfidf <- clean_allcsv2$Job_Description
# Control list to be used for all corpuses
control_list <- list(removePunctuation = TRUE, stopwords = TRUE, tolower = TRUE,
weighting = weightTfIdf)
#tfidf <- clean_allcsv1
tfidf <- clean_allcsv$Job_Description
# Control list to be used for all corpuses
control_list <- list(removePunctuation = TRUE, stopwords = TRUE, tolower = TRUE,
weighting = weightTfIdf)
#tfidf <- clean_allcsv1
tfidf <- clean_allcsv$Job_Description
# Control list to be used for all corpuses
control_list <- list(removePunctuation = TRUE, stopwords = TRUE, tolower = TRUE,
weighting = weightTfIdf)
head(tfidf)
#tfidf <- clean_allcsv1
tfidf <- clean_allcsv$Job_Description
# Control list to be used for all corpuses
control_list <- list(removePunctuation = TRUE, stopwords = TRUE, tolower = TRUE,
weighting = weightTfIdf)
colnames(tfidf)
#tfidf <- clean_allcsv1
tfidf <- clean_allcsv
# Control list to be used for all corpuses
control_list <- list(removePunctuation = TRUE, stopwords = TRUE, tolower = TRUE,
weighting = weightTfIdf)
colnames(tfidf)
# TF-IDF Based on the 3 job postings
corpus.all <- VCorpus(VectorSource(tfidf$Job_Description))
dtm.all <- DocumentTermMatrix(corpus.all, control = control_list)
dtm.all
word_cloud <- tm_map(corpus.all, removeWords, c("services", "data", "andor", "ability", "using", "new", "science", "scientist" , "you", "must", "will", "including", "can", stopwords('english')))
#tfidf <- clean_allcsv1
tfidf <- clean_allcsv
# Control list to be used for all corpuses
control_list <- list(removePunctuation = TRUE, stopwords = TRUE,
weighting = weightTfIdf)
colnames(tfidf)
# TF-IDF Based on the 3 job postings
corpus.all <- VCorpus(VectorSource(tfidf$Job_Description))
dtm.all <- DocumentTermMatrix(corpus.all, control = control_list)
dtm.all
knitr::opts_chunk$set(echo = TRUE)
# Load packages
library(rvest)
library(stringr)
library(dplyr)
library(ggplot2)
library(tidyverse)
library(xml2)
library(httr)
library("tm")
library("SnowballC")
library("wordcloud")
library("RColorBrewer")
library(tidytext)
library("magrittr")
require(ggthemes)
library('sentimentr')
#read from Leticia
ltcancel<-read.csv("https://raw.githubusercontent.com/ltcancel/Project3/master/SimplyHiredJobs.csv")
colnames(ltcancel)<-c("Position", "Company","Location","Salary","URL","Job_Description")
#read from Salma
selshahawy<-read.csv("https://raw.githubusercontent.com/salma71/MSDS_2019/master/Fall2019/aquisition_management_607/project_3/jobs_detailsInfo.csv", stringsAsFactors = FALSE)
colnames(selshahawy)<-c("Position", "Company","Location","URL","Job_Description")
#read from Sufian
ssufian<-read.csv("https://raw.githubusercontent.com/Luz917/data607project3_ssufian_monster_jobs/master/monsterjobs.csv", stringsAsFactors = FALSE)
colnames(ssufian)<-c("Position", "Company","Location","Salary","URL","Job_Description")
twocsv<-merge(ltcancel,selshahawy,all= TRUE)
allcsv<-merge(twocsv,ssufian, all=TRUE)
library("magrittr")
# REmove Salary column
clean_allcsv <- allcsv[,-c(4,6)]
#tfidf <- clean_allcsv1
tfidf <- clean_allcsv
# Control list to be used for all corpuses
control_list <- list(removePunctuation = TRUE, stopwords = TRUE,
weighting = weightTfIdf)
colnames(tfidf)
# TF-IDF Based on the 3 job postings
corpus.all <- VCorpus(VectorSource(tfidf$Job_Description))
dtm.all <- DocumentTermMatrix(corpus.all, control = control_list)
# TF-IDF Based on the 3 job postings
corpus.all <- VCorpus(VectorSource(tfidf$Job_Description))
dtm.all <- DocumentTermMatrix(corpus.all)
# TF-IDF Based on the 3 job postings
corpus.all <- VCorpus(VectorSource(tfidf$Job_Description))
dtm.all <- DocumentTermMatrix(corpus.all)
#tfidf <- clean_allcsv1
tfidf <- clean_allcsv
# Control list to be used for all corpuses
control_list <- list(removePunctuation = TRUE, stopwords = TRUE, tolower = TRUE,
weighting = weightTfIdf)
colnames(tfidf)
# TF-IDF Based on the 3 job postings
corpus.all <- VCorpus(VectorSource(tfidf$Job_Description))
dtm.all <- DocumentTermMatrix(corpus.all)
# Load packages
library(rvest)
library(stringr)
library(dplyr)
library(ggplot2)
library(tidyverse)
library(xml2)
library(httr)
library("tm")
library("SnowballC")
library("wordcloud")
library("RColorBrewer")
library(tidytext)
library("magrittr")
require(ggthemes)
library('sentimentr')
#read from Leticia
ltcancel<-read.csv("https://raw.githubusercontent.com/ltcancel/Project3/master/SimplyHiredJobs.csv")
colnames(ltcancel)<-c("Position", "Company","Location","Salary","URL","Job_Description")
#read from Salma
selshahawy<-read.csv("https://raw.githubusercontent.com/salma71/MSDS_2019/master/Fall2019/aquisition_management_607/project_3/jobs_detailsInfo.csv", stringsAsFactors = FALSE)
colnames(selshahawy)<-c("Position", "Company","Location","URL","Job_Description")
#read from Sufian
ssufian<-read.csv("https://raw.githubusercontent.com/Luz917/data607project3_ssufian_monster_jobs/master/monsterjobs.csv", stringsAsFactors = FALSE)
colnames(ssufian)<-c("Position", "Company","Location","Salary","URL","Job_Description")
twocsv<-merge(ltcancel,selshahawy,all= TRUE)
allcsv<-merge(twocsv,ssufian, all=TRUE)
library("magrittr")
# REmove Salary column
clean_allcsv <- allcsv[,-c(4,6)]
#tfidf <- clean_allcsv1
tfidf <- clean_allcsv
# Control list to be used for all corpuses
control_list <- list(removePunctuation = TRUE, stopwords = TRUE, tolower = TRUE,
weighting = weightTfIdf)
colnames(tfidf)
head(tfidf)
corpus.all <- VCorpus(VectorSource(tfidf$Job_Description))
corpus.all
dtm.all <- DocumentTermMatrix(corpus.all)
dtm.all <- DocumentTermMatrix(corpus.all, control = control_list)
#tfidf <- clean_allcsv1
tfidf <- iconv(clean_allcsv,"WINDOWS-1252","UTF-8")
# Control list to be used for all corpuses
control_list <- list(removePunctuation = TRUE, stopwords = TRUE, tolower = TRUE,
weighting = weightTfIdf)
colnames(tfidf)
#tfidf <- clean_allcsv1
tfidf <- clean_allcsv
# Control list to be used for all corpuses
control_list <- list(removePunctuation = TRUE, stopwords = TRUE, tolower = TRUE,
weighting = weightTfIdf)
colnames(tfidf)
clean_allcsv[,sapply(clean_allcsv,is.character)] <- sapply(
clean_allcsv[,sapply(clean_allcsv,is.character)],
iconv,"WINDOWS-1252","UTF-8")
clean_allcsv
head(clean_allcsv)
tfidf <- clean_allcsv
corpus.all <- VCorpus(VectorSource(tfidf$Job_Description))
dtm.all <- DocumentTermMatrix(corpus.all)
dtm.all <- DocumentTermMatrix(corpus.all, control = control_list)
library("magrittr")
# REmove Salary column
clean_allcsv <- allcsv[,-c(4,6)]
clean_allcsv <- write.csv(clean_allcsv, "data_input.csv", fileEncoding = "UTF-8")
library("magrittr")
# REmove Salary column
clean_allcsv <- allcsv[,-c(4,6)]
clean_allcsv
is.data.frame(tfidf)
# TF-IDF Based on the 3 job postings
corpus.all <- VCorpus(VectorSource(tfidf$Job_Description))
dtm.all <- DocumentTermMatrix(corpus.all)
tfidf$Job_Description <- iconv(enc2utf8(tfidf$Job_Description), sub = "byte")
tm_map(tfidf, function(x) iconv(enc2utf8(x$Job_Description), sub = "bytes"))
tm_map(tfidf$Job_Description, function(x) iconv(enc2utf8(x$Job_Description), sub = "bytes"))
tm_map(corpus.all, function(x) iconv(enc2utf8(x$Job_Description), sub = "bytes"))
tm_map(tfidf$Job_Description, function(x) iconv(enc2utf8(x$Job_Description), sub = "bytes"))
#read from Leticia
ltcancel<-read.csv("https://raw.githubusercontent.com/ltcancel/Project3/master/SimplyHiredJobs.csv", stringsAsFactors = FALSE, fileEncoding="latin1")
colnames(ltcancel)<-c("Position", "Company","Location","Salary","URL","Job_Description")
#read from Salma
selshahawy<-read.csv("https://raw.githubusercontent.com/salma71/MSDS_2019/master/Fall2019/aquisition_management_607/project_3/jobs_detailsInfo.csv", stringsAsFactors = FALSE, fileEncoding="latin1")
colnames(selshahawy)<-c("Position", "Company","Location","URL","Job_Description")
#read from Sufian
ssufian<-read.csv("https://raw.githubusercontent.com/Luz917/data607project3_ssufian_monster_jobs/master/monsterjobs.csv", stringsAsFactors = FALSE, fileEncoding="latin1")
colnames(ssufian)<-c("Position", "Company","Location","Salary","URL","Job_Description")
word_cloud <- tm_map(corpus.all, removeWords, c("services", "data", "andor", "ability", "using", "new", "science", "scientist" , "you", "must", "will", "including", "can", stopwords('english')))
twocsv<-merge(ltcancel,selshahawy,all= TRUE)
allcsv<-merge(twocsv,ssufian, all=TRUE)
library("magrittr")
# REmove Salary column
clean_allcsv <- allcsv[,-c(4,6)]
#tfidf <- clean_allcsv1
tfidf <- clean_allcsv
# Control list to be used for all corpuses
control_list <- list(removePunctuation = TRUE, stopwords = TRUE, tolower = TRUE,
weighting = weightTfIdf)
colnames(tfidf)
# TF-IDF Based on the 3 job postings
corpus.all <- VCorpus(VectorSource(tfidf$Job_Description))
dtm.all <- DocumentTermMatrix(corpus.all)
dtm.all
tdm.all <- TermDocumentMatrix(corpus.all)
inspect(tdm.all)
head(scores1)
colnames(sentiment_score_df)
colnames(clean_allcsv)
scores1
scores1 <- sentiment_by(clean_allcsv$Job_Description, by=NULL) %>%
pull(ave_sentiment)
head(scores1)
#m_wide2 <- m_wide
#clean_allcsv2$sentiment_scores<- scores1
#clean_allcsv3 <- clean_allcsv2[,-c(4)] # deleting redundant column
sentiment_score_df <- clean_allcsv  %>%
select(Company, sentiment_scores)
scores1 <- sentiment_by(clean_allcsv$Job_Description) %>%
pull(ave_sentiment)
head(scores1)
#m_wide2 <- m_wide
#clean_allcsv2$sentiment_scores<- scores1
#clean_allcsv3 <- clean_allcsv2[,-c(4)] # deleting redundant column
sentiment_score_df <- clean_allcsv  %>%
select(Company, sentiment_scores)
colnames(clean_allcsv)
colnames(clean_allcsv2)
scores1 <- sentiment_by(clean_allcsv2$Job_Description) %>%
pull(ave_sentiment)
clean_allcsv2 <- clean_allcsv[,-c(4)] # this is for sentinment later
scores1 <- sentiment_by(clean_allcsv2$Job_Description) %>%
pull(ave_sentiment)
clean_allcsv2 <- clean_allcsv[,-c(4)] # this is for sentinment later
scores1 <- sentiment_by(clean_allcsv2$Job_Description, by = NULL) %>%
pull(ave_sentiment)
colnames(clean_allcsv2)
colnames(clean_allcsv)
clean_allcsv2 <- clean_allcsv # this is for sentinment later
scores1 <- sentiment_by(clean_allcsv2$Job_Description, by = NULL) %>%
pull(ave_sentiment)
head(scores1)
#m_wide2 <- m_wide
#clean_allcsv2$sentiment_scores<- scores1
#clean_allcsv3 <- clean_allcsv2[,-c(4)] # deleting redundant column
sentiment_score_df <- clean_allcsv  %>%
select(Company, sentiment_scores)
colnames(clean_allcsv2)
clean_allcsv2 <- clean_allcsv # this is for sentinment later
scores1 <- sentiment_by(clean_allcsv2$Job_Description, by = NULL) %>%
pull(ave_sentiment)
head(scores1)
#m_wide2 <- m_wide
#clean_allcsv2$sentiment_scores<- scores1
#clean_allcsv3 <- clean_allcsv2[,-c(4)] # deleting redundant column
sentiment_score_df <- clean_allcsv2  %>%
select(Company, sentiment_scores)
colnames(clean_allcsv2)
scores1 <- sentiment_by(clean_allcsv2$Job_Description, by = NULL) %>%
pull(ave_sentiment)
colnames(scores1)
sentiment_score_df
clean_allcsv2 <- clean_allcsv # this is for sentinment later
scores1 <- sentiment_by(clean_allcsv2$Job_Description, by = NULL) %>%
pull(ave_sentiment)
head(scores1)
head(sentiment_score_df)
colnames(clean_allcsv2)
knitr::opts_chunk$set(echo = TRUE)
require(dplyr)
require(rvest)
require(stringr)
require(tm)
require(SnowballC)
require(tidytext)
require(stringr)
require(textdata)
require(tidyverse)
require(ggplot2)
require(wordcloud)
require(widyr)
require(igraph)
require(ggraph)
require(kableExtra)
ltcancel<-read.csv("https://raw.githubusercontent.com/ltcancel/Project3/master/SimplyHiredJobs.csv", stringsAsFactors = FALSE)
colnames(ltcancel)<-c("Position", "Company","Location","Salary","URL","Job_Description")
str(ltcancel)
selshahawy<-read.csv("https://raw.githubusercontent.com/salma71/MSDS_2019/master/Fall2019/aquisition_management_607/project_3/jobs_detailsInfo.csv", stringsAsFactors = FALSE)
colnames(selshahawy)<-c("Position", "Company","Location","URL","Job_Description")
str(selshahawy)
ssufian<-read.csv("https://raw.githubusercontent.com/salma71/DataScience_skills/master/monsterjobs.csv", stringsAsFactors = FALSE)
install.packages(c("ggraph", "igraph", "textdata", "widyr"))
colnames(ssufian)<-c("Position", "Company","Location","Salary","URL","Job_Description")
str(ssufian)
twocsv<-merge(ltcancel,selshahawy,all= TRUE)
str(twocsv)
allcsv<-merge(twocsv,ssufian, all=TRUE)
str(allcsv)
allcsv2 <-allcsv[c(1,2,5)]
descriptionofjobs = Corpus(VectorSource(allcsv2$Job_Description))
descriptionofjobs = tm_map(descriptionofjobs, content_transformer(tolower))##changes to lower letters
knitr::opts_chunk$set(echo = TRUE)
require(dplyr)
require(rvest)
require(stringr)
require(tm)
require(SnowballC)
require(tidytext)
require(stringr)
require(textdata)
require(tidyverse)
require(ggplot2)
require(wordcloud)
require(widyr)
require(igraph)
require(ggraph)
require(kableExtra)
ltcancel<-read.csv("https://raw.githubusercontent.com/ltcancel/Project3/master/SimplyHiredJobs.csv", stringsAsFactors = FALSE)
colnames(ltcancel)<-c("Position", "Company","Location","Salary","URL","Job_Description")
str(ltcancel)
selshahawy<-read.csv("https://raw.githubusercontent.com/salma71/MSDS_2019/master/Fall2019/aquisition_management_607/project_3/jobs_detailsInfo.csv", stringsAsFactors = FALSE)
colnames(selshahawy)<-c("Position", "Company","Location","URL","Job_Description")
str(selshahawy)
ssufian<-read.csv("https://raw.githubusercontent.com/salma71/DataScience_skills/master/monsterjobs.csv", stringsAsFactors = FALSE)
colnames(ssufian)<-c("Position", "Company","Location","Salary","URL","Job_Description")
str(ssufian)
twocsv<-merge(ltcancel,selshahawy,all= TRUE)
str(twocsv)
allcsv<-merge(twocsv,ssufian, all=TRUE)
str(allcsv)
allcsv2 <-allcsv[c(1,2,5)]
descriptionofjobs = Corpus(VectorSource(allcsv2$Job_Description))
descriptionofjobs = tm_map(descriptionofjobs, content_transformer(tolower))##changes to lower letters
knitr::opts_chunk$set(echo = TRUE)
require(dplyr)
require(rvest)
require(stringr)
require(tm)
require(SnowballC)
require(tidytext)
require(stringr)
require(textdata)
require(tidyverse)
require(ggplot2)
require(wordcloud)
require(widyr)
require(igraph)
require(ggraph)
require(kableExtra)
ltcancel<-read.csv("https://raw.githubusercontent.com/ltcancel/Project3/master/SimplyHiredJobs.csv", stringsAsFactors = FALSE, fileEncoding="latin1")
colnames(ltcancel)<-c("Position", "Company","Location","Salary","URL","Job_Description")
str(ltcancel)
selshahawy<-read.csv("https://raw.githubusercontent.com/salma71/MSDS_2019/master/Fall2019/aquisition_management_607/project_3/jobs_detailsInfo.csv", stringsAsFactors = FALSE, fileEncoding="latin1")
colnames(selshahawy)<-c("Position", "Company","Location","URL","Job_Description")
str(selshahawy)
ssufian<-read.csv("https://raw.githubusercontent.com/salma71/DataScience_skills/master/monsterjobs.csv", stringsAsFactors = FALSE, fileEncoding="latin1")
colnames(ssufian)<-c("Position", "Company","Location","Salary","URL","Job_Description")
str(ssufian)
twocsv<-merge(ltcancel,selshahawy,all= TRUE)
str(twocsv)
allcsv<-merge(twocsv,ssufian, all=TRUE)
str(allcsv)
allcsv2 <-allcsv[c(1,2,5)]
descriptionofjobs = Corpus(VectorSource(allcsv2$Job_Description))
descriptionofjobs = tm_map(descriptionofjobs, content_transformer(tolower))##changes to lower letters
descriptionofjobs= tm_map(descriptionofjobs, content_transformer(gsub), pattern="\\W",replace=" ")
removeURL = function(x) gsub("http^\\s\\s*", "", x)%>%
descriptionofjobs = tm_map(descriptionofjobs, content_transformer(removeURL))
descriptionofjobs=tm_map(descriptionofjobs,removeNumbers) ##Remove numbers
descriptionofjobs=tm_map(descriptionofjobs,removePunctuation)##Punctuation
descriptionofjobs = tm_map(descriptionofjobs, removeWords, stopwords(kind = "english"))##Stopwords
extraStopwords = c(setdiff(stopwords('english'), c("used", "will")), "time", "can", "sex", "role", "new","can", "job", "etc", "one", "looking", "well","use","best","also", "high", "real", "please", "key", "able", "must", "like", "full", "include", "good", "non", "need","plus","day","year", "com", "want", "age","using","sexual", "help","apply", "race", "orientation", "will", "work", "new")
descriptionofjobs = tm_map(descriptionofjobs, removeWords, extraStopwords) ##more stop words or unwanted words
descriptionofjobs = tm_map (descriptionofjobs, stripWhitespace)
allwords2<-DocumentTermMatrix(descriptionofjobs)
sparsewords = removeSparseTerms(allwords2,.50)
tidywords<-tidy(sparsewords)
tidywords
totalwords<-tidywords%>%
count(term, sort= TRUE)
kable(totalwords) %>%
kable_styling(full_width = F) %>%
column_spec(1, bold = T, border_right = F) %>%
column_spec(1, width = "15em", background = "lightgreen")
tidywords %>%
count(term, sort = TRUE) %>%
filter(n > 180) %>%
ggplot(aes(term, n)) +
geom_col() +
xlab(NULL) +
coord_flip()
summaryofwords<-tidywords %>%
group_by(term) %>%
summarize(total = sum(n()))%>%
arrange(desc(total))
totalwords<-left_join(totalwords, summaryofwords)
tfrequency <-summaryofwords %>%
group_by(term)%>%
mutate(rank= n(), 'frequencyofterm' = n()/total)%>%
arrange(desc(total))
kable(tfrequency) %>%
kable_styling(full_width = F) %>%
column_spec(1, bold = T, border_right = F) %>%
column_spec(1, width = "15em", background = "cyan")
tidy_word_pairs<-tidywords%>%
pairwise_count(term,count, sort= TRUE)
colnames(tidywords)
dim(tidywords)
tidy_word_pairs
tidywords%>%
pairwise_count(term,count, sort= TRUE)
tidywords%>%
pairwise_count(count, sort= TRUE)
tidywords%>%
pairwise_count(term, sort= TRUE)
tidywords%>%
pairwise_count(term,count, sort= TRUE)
tidywords%>%
pairwise_count(term,count)
tidywords%>%
pairwise_count(term,count, sort= TRUE)
colnames(tidywords)
tidywords%>%
group_by(term)
tidy_word_pairs<-tidywords%>%
group_by(term) %>%
arrange()
kable(tidy_word_pairs [1:20, 1:3]) %>%
kable_styling(full_width = F) %>%
column_spec(1, bold = T, border_right = F) %>%
column_spec(2, bold = T, border_right = F)%>%
column_spec(1, width = "10em", background = "yellow")%>%
column_spec(2, width = "10em", background = "cyan")
tidy_word_pairs<-tidywords%>%
group_by(term) %>%
arrange(count)
kable(tidy_word_pairs [1:20, 1:3]) %>%
kable_styling(full_width = F) %>%
column_spec(1, bold = T, border_right = F) %>%
column_spec(2, bold = T, border_right = F)%>%
column_spec(1, width = "10em", background = "yellow")%>%
column_spec(2, width = "10em", background = "cyan")
set.seed(1234)
tidy_word_pairs %>%
filter(n >= 8) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "cyan3") +
geom_node_point(size = 5) +
geom_node_text(aes(label = name), repel = TRUE,
point.padding = unit(0.2, "lines")) +
theme_void()
#set.seed(1234)
tidy_word_pairs %>%
filter(n >= 8) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "cyan3") +
geom_node_point(size = 5) +
geom_node_text(aes(label = name), repel = TRUE,
point.padding = unit(0.2, "lines")) +
theme_void()
#set.seed(1234)
tidy_word_pairs %>%
filter(count >= 8) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "cyan3") +
geom_node_point(size = 5) +
geom_node_text(aes(label = name), repel = TRUE,
point.padding = unit(0.2, "lines")) +
theme_void()
#set.seed(1234)
tidy_word_pairs %>%
filter(count >= 8) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = count, edge_width = count), edge_colour = "cyan3") +
geom_node_point(size = 5) +
geom_node_text(aes(label = name), repel = TRUE,
point.padding = unit(0.2, "lines")) +
theme_void()
tidywords_cors <- tidywords %>%
group_by(term) %>%
filter(n() >= 180) %>%
pairwise_cor(term, count, sort = TRUE, upper = FALSE)
kable(tidywords_cors) %>%
kable_styling(full_width = F) %>%
column_spec(1, bold = T, border_right = F) %>%
column_spec(2, bold = T, border_right = F)%>%
column_spec(1, width = "10em", background = "cyan")%>%
column_spec(2, width = "10em", background = "yellow")
set.seed(1234)
tidywords_cors %>%
filter(correlation > .3) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = correlation, edge_width = correlation), edge_colour = "lightblue2") +
geom_node_point(size = 5) +
geom_node_text(aes(label = name), repel = TRUE,
point.padding = unit(0.3, "lines")) +
theme_void()
library(wordcloud)
dtm = DocumentTermMatrix(descriptionofjobs)
dtm = removeSparseTerms(dtm, 0.65)
dataset = as.matrix(dtm)
v = sort(colSums(dataset),decreasing=TRUE)
myNames = names(v)
d = data.frame(word=myNames,freq=v)
wordcloud(d$word, colors=c(1:4),random.color=TRUE, d$freq, min.freq=100)
csv123<-df_list<- list(ltcancel,selshahawy,ssufian)
Reduce(function(d1,d2) merge(d1, d2, by = "Position",all.x=TRUE, all.y = FALSE),df_list)
summary(csv123)
