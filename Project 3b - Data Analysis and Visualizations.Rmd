---
title: "Project 3b - Data Analysis and Visualizations"
author: "Sufian"
date: "10/16/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r message=FALSE, warning=FALSE}

# Load packages
library(rvest)
library(stringr)
library(dplyr)
library(ggplot2)
library(tidyverse)
library(xml2)
library(httr)
library("tm")
library("SnowballC")
library("wordcloud")
library("RColorBrewer")
library(tidytext)
library("magrittr")



```

## R Markdown


```{r}
#read from Leticia
ltcancel<-read.csv("https://raw.githubusercontent.com/ltcancel/Project3/master/SimplyHiredJobs.csv")
colnames(ltcancel)<-c("Position", "Company","Location","Salary","URL","Job_Description")

#read from Salma
selshahawy<-read.csv("https://raw.githubusercontent.com/salma71/MSDS_2019/master/Fall2019/aquisition_management_607/project_3/jobs_detailsInfo.csv", stringsAsFactors = FALSE) 
colnames(selshahawy)<-c("Position", "Company","Location","URL","Job_Description") 

#read from Sufian
ssufian<-read.csv("https://raw.githubusercontent.com/Luz917/data607project3_ssufian_monster_jobs/master/monsterjobs.csv", stringsAsFactors = FALSE) 
colnames(ssufian)<-c("Position", "Company","Location","Salary","URL","Job_Description")  


```

## Including Plots

You can also embed plots, for example:

```{r }
twocsv<-merge(ltcancel,selshahawy,all= TRUE)

allcsv<-merge(twocsv,ssufian, all=TRUE)

head(allcsv)
```




```{r}
library("magrittr")
# REmove Salary column

clean_allcsv <- allcsv[,-c(4,6)]

head(clean_allcsv)



 

 
```


```{r}

# Removed brackets surrounding summaries

clean_allcsv <- clean_allcsv %>% mutate_each(funs(tolower), Position, Company, Location, Job_Description)

clean_allcsv1 <- str_replace_all(clean_allcsv$Job_Description, "[\r\n]" , "")
clean_allcsv1 <- str_replace_all(clean_allcsv$Job_Description, "\\." , "")
clean_allcsv1 <- str_replace_all(clean_allcsv$Job_Description, "\\*" , "")
clean_allcsv1 <- str_replace_all(clean_allcsv$Job_Description, "\\+" , "")
clean_allcsv1 <- str_replace_all(clean_allcsv$Job_Description, "\\\\" , "")
clean_allcsv1 <- str_replace_all(clean_allcsv$Job_Description, "\\)" , "")
clean_allcsv1 <- str_replace_all(clean_allcsv$Job_Description, "\\(" , "")
clean_allcsv1 <- str_replace_all(clean_allcsv$Job_Description, "[[:punct:]]+", "")
clean_allcsv1 <- str_replace_all(clean_allcsv$Job_Description, "[\n]" , "")


head(clean_allcsv1 )
              
clean_allcsv2 <- clean_allcsv[,-c(4)] # this is for sentinment later


clean_allcsv2$Job_Description <- clean_allcsv1 
  
```



```{r}
#tfidf <- clean_allcsv1
tfidf <- clean_allcsv2$Job_Description 

# Control list to be used for all corpuses
control_list <- list(removePunctuation = TRUE, stopwords = TRUE, tolower = TRUE,
                     weighting = weightTfIdf)

```

```{r}
# TF-IDF Based on the 3 job postings

corpus.all <- VCorpus(VectorSource(tfidf))
tdm.all <- TermDocumentMatrix(corpus.all, control = control_list)
inspect(tdm.all)
# Remove outliers consisting of very rare terms
tdm.40 <- removeSparseTerms(tdm.all, sparse = 0.40)
inspect(tdm.40)
# Sum rows for total & make dataframe
df_all <- tidy(sort(rowSums(as.matrix(tdm.40))))
colnames(df_all) <- c("words", "count")
View(df_all)


```




```{r}


# Graph
ggplot(tail(df_all, 25), aes(reorder(words, count), count)) +
  geom_bar(stat = "identity", fill = "blue") +
  labs(title = "TF-IDF of 3 Job Postings",
       x = "Words", y = "Frequency") +
  coord_flip()

```
```{r}
docs <- Corpus(VectorSource(clean_allcsv2$Job_Description))
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs  , toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")

# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
# Text stemming
docs <- tm_map(docs, stemDocument)

inspect(docs)
dtmw <- TermDocumentMatrix(docs )
m <- as.matrix(dtmw)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)

```

### Generate the Word cloud

The importance of key skill sets can be illustrated as a word cloud as follow :


```{r}
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
          max.words=200, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
```




```{r}

#new control list
control_list2 <- list(removePunctuation = TRUE, stopwords = TRUE, tolower = TRUE)


dtm_original<- DocumentTermMatrix(corpus.all,control_list2) 

tdm_original.40 <- removeSparseTerms( dtm_original, sparse = 0.40)


inspect(dtm_original)
inspect(tdm_original.40)


findAssocs(tdm_original.40,"opec", 0.5) #get high correlation words
findFreqTerms(dtm_original, 5)

```

```{r}
data(stop_words)
d <- tibble(text=clean_allcsv2$Job_Description )

d %>% 
  unnest_tokens(word, text)

tidy_books <- d %>%
  anti_join(stop_words)

d %>% count(word, sort = TRUE)



```

