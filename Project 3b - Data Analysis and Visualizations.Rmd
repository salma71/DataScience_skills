title: "Project 3b - Data Analysis and Visualizations"

author: "Sufian"

date: "10/16/2019"

output: html_document

--------------------------------------------------------------------------------

\clearpage


Rpub links:

http://rpubs.com/ssufian/540285

--------------------------------------------------------------------------------

\clearpage


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r message=FALSE, warning=FALSE}
# Load packages
library(rvest)
library(stringr)
library(dplyr)
library(ggplot2)
library(tidyverse)
library(xml2)
library(httr)
library("tm")
library("SnowballC")
library("wordcloud")
library("RColorBrewer")
library(tidytext)
library("magrittr")
require(ggthemes)
library('sentimentr')
```

## Reading each teammates' job site scrapped job listings

```{r}
#read from Leticia
ltcancel<-read.csv("https://raw.githubusercontent.com/ltcancel/Project3/master/SimplyHiredJobs.csv", stringsAsFactors = FALSE, fileEncoding="latin1")
colnames(ltcancel)<-c("Position", "Company","Location","Salary","URL","Job_Description")
#read from Salma
selshahawy<-read.csv("https://raw.githubusercontent.com/salma71/MSDS_2019/master/Fall2019/aquisition_management_607/project_3/jobs_detailsInfo.csv", stringsAsFactors = FALSE, fileEncoding="latin1") 
colnames(selshahawy)<-c("Position", "Company","Location","URL","Job_Description") 
#read from Sufian
ssufian<-read.csv("https://raw.githubusercontent.com/Luz917/data607project3_ssufian_monster_jobs/master/monsterjobs.csv", stringsAsFactors = FALSE, fileEncoding="latin1") 
colnames(ssufian)<-c("Position", "Company","Location","Salary","URL","Job_Description")  
```

### Merging all scrapped files into one


```{r }
twocsv<-merge(ltcancel,selshahawy,all= TRUE)
allcsv<-merge(twocsv,ssufian, all=TRUE)
```

### Deleting unnecessary columns

```{r}
library("magrittr")
# REmove Salary column
clean_allcsv <- allcsv[,-c(4,6)]
```

### Data cleaning using REGEX


```{r}
#tfidf <- clean_allcsv1
tfidf <- clean_allcsv
# Control list to be used for all corpuses
control_list <- list(removePunctuation = TRUE, stopwords = TRUE, tolower = TRUE,
                     weighting = weightTfIdf)
colnames(tfidf)
```

```{r}
# TF-IDF Based on the 3 job postings
corpus.all <- VCorpus(VectorSource(tfidf$Job_Description))
dtm.all <- DocumentTermMatrix(corpus.all)
dtm.all
```

```{r}
tdm.all <- TermDocumentMatrix(corpus.all)
inspect(tdm.all)
```



```{r}
# Remove outliers consisting of very rare terms - infrequent words outlairs
tdms.60 <- removeSparseTerms(tdm.all, sparse = 0.60)
inspect(tdms.60)
```

```{r}
# explore the data
# The data reduction techniques used singular value decomposition reduces the number of columns (documents) but keeps the number of rows (words).
count <- rowSums(as.matrix(tdms.60))

count <- sort(rowSums(as.matrix(tdms.60)), decreasing=TRUE)   
head(count, 14)
```

```{r}
df_all <- data.frame(words=names(count), count=count)   
head(df_all)
```


```{r}
# Graph of TF-IDF in the 3 job postings
ggplot(head(df_all,25), aes(reorder(words, count), count)) +  # head because I desc the order 
  geom_bar(stat = "identity", fill = "blue") +
  labs(title = "TF-IDF analysis based on TermDocumentMatrix",
       subtitle = "Analysis based on three job postings sites",
       x = "Words", y = "Frequency") +
  coord_flip()
```

### Text Mining and Word Cloud by building a term-document matrix


```{r}
## here we wanted to plot word cloud to get the most used words regarding to TDF 
word_cloud <- tm_map(corpus.all, removeWords, c("services", "data", "andor", "ability", "using", "new", "science", "scientist" , "you", "must", "will", "including", "can", stopwords('english')))

```

```{r}
wordcloud(names(count), 
          count,
          max.words = 80, 
          random.order = FALSE, 
          scale=c(2.5,.4),
          random.color = FALSE,
          colors=palette())
```


### Generate the Word cloud

The important key skill sets can be illustrated in a word cloud as follow :

--------------------------------------------------------------------------------

\clearpage

### Explore frequent terms and their associations

--------------------------------------------------------------------------------

\clearpage


### Frequency of Technical skills

```{r}
toolskills <- clean_allcsv %>%
    mutate(R = grepl("\\bR\\b,", Job_Description, ignore.case=TRUE)) %>%
    mutate(python = grepl("Python", Job_Description, ignore.case=TRUE)) %>%
    mutate(SQL = grepl("SQL", Job_Description, ignore.case=TRUE)) %>%
    mutate(hadoop = grepl("hadoop", Job_Description, ignore.case=TRUE)) %>%
    mutate(perl = grepl("perl", Job_Description, ignore.case=TRUE)) %>%
    mutate(matplotlib = grepl("matplotlib", Job_Description, ignore.case=TRUE)) %>%
    mutate(Cplusplus = grepl("C++", Job_Description, fixed=TRUE)) %>%
    mutate(VB = grepl("VB", Job_Description, ignore.case=TRUE)) %>%
    mutate(java = grepl("java\\b", Job_Description, ignore.case=TRUE)) %>%
    mutate(scala = grepl("scala", Job_Description, ignore.case=TRUE)) %>%
    mutate(tensorflow = grepl("tensorflow",Job_Description, ignore.case=TRUE)) %>%
    mutate(javascript = grepl("javascript", Job_Description, ignore.case=TRUE)) %>%
    mutate(spark = grepl("spark", Job_Description, ignore.case=TRUE)) %>%
    mutate(java = grepl("java", Job_Description, ignore.case=TRUE)) %>%
    select(
      Position, 
      Company, 
      R, 
      python, 
      SQL, 
      hadoop, 
      perl, 
      matplotlib, 
      Cplusplus, 
      VB, 
      java, 
      scala, 
      tensorflow, 
      javascript,
      spark,
      java)
```

### Setup tool skills for plotting

```{r}
toolskills2 <- toolskills %>% select(-(1:2)) %>% summarise_all(sum) %>% gather(variable,value) %>% arrange(desc(value))
```

### Visualized the most in-demand tool sKills:

```{r}
ggplot(toolskills2,aes(x=reorder(variable, value), y=value)) + geom_bar(stat='identity',fill="green",color="black") + xlab('') + ylab('Frequency') + labs(title='Tool Skills') + coord_flip() + theme_minimal()
```

### Frequency of Hard skills

```{r}
hardskills <- clean_allcsv %>%
    mutate(machinelearning = grepl("machine learning", Job_Description, ignore.case=TRUE)) %>%
    mutate(modeling = grepl("model", Job_Description, ignore.case=TRUE)) %>%
    mutate(statistics = grepl("statistics", Job_Description, ignore.case=TRUE)) %>%
    mutate(programming = grepl("programming", Job_Description, ignore.case=TRUE)) %>%
    mutate(quantitative = grepl("quantitative", Job_Description, ignore.case=TRUE)) %>%
    mutate(debugging = grepl("debugging", Job_Description, ignore.case=TRUE)) %>%
    mutate(statistical = grepl("statistical",  Job_Description, ignore.case=TRUE)) %>%
    mutate(regression = grepl("regression",  Job_Description, ignore.case=TRUE)) %>%
  
select(Position, Company, machinelearning, modeling, statistics, programming, quantitative, debugging, statistical, regression)
```

### Setup hard skills for plotting

```{r}

hardskills2 <- hardskills %>% 
               select(-(1:2)) %>% 
               summarise_all(sum) %>% 
               gather(variable,value) %>% 
               arrange(desc(value))
```

### Visualized the most in-demand hard sKills:

```{r}
ggplot(hardskills2,aes(x=reorder(variable, value), y=value)) + 
  geom_bar(stat='identity',fill="yellow",color="black") + 
  xlab('') + 
  ylab('Frequency') + 
  labs(title='Hard Skills') + 
  coord_flip() + 
  theme_minimal()
```

### Frequency of Soft skills

```{r}
softskills <- clean_allcsv %>%
    mutate(workingremote = grepl("working remote", Job_Description, ignore.case=TRUE)) %>% 
    mutate(communication = grepl("communication", Job_Description, ignore.case=TRUE)) %>%
    mutate(collaborative = grepl("collaborate", Job_Description, ignore.case=TRUE)) %>%
    mutate(creative = grepl("creative", Job_Description, ignore.case=TRUE)) %>%
    mutate(critical = grepl("critical", Job_Description, ignore.case=TRUE)) %>%
    mutate(problemsolving = grepl("problem solving", Job_Description, ignore.case=TRUE)) %>%
    mutate(activelearning = grepl("active learning", Job_Description, ignore.case=TRUE)) %>%
    mutate(hypothesis = grepl("hypothesis", Job_Description, ignore.case=TRUE)) %>%
    mutate(organized = grepl("organize", Job_Description, ignore.case=TRUE)) %>%
    mutate(judgement = grepl("judgement", Job_Description, ignore.case=TRUE)) %>%
    mutate(selfstarter = grepl("self Starter", Job_Description, ignore.case=TRUE)) %>%
    mutate(interpersonalskills = grepl("interpersonal skills", Job_Description, ignore.case=TRUE)) %>%
    mutate(detail_oriented = grepl("attention to detail", Job_Description, ignore.case=TRUE)) %>%
    mutate(visualization = grepl("visualization", Job_Description, ignore.case=TRUE)) %>%
    mutate(leadership = grepl("leadership", Job_Description, ignore.case=TRUE)) %>%
    mutate(presentation = grepl("presentation", Job_Description, ignore.case=TRUE)) %>%
    mutate(passion = grepl("passion", Job_Description, ignore.case=TRUE)) %>%
    mutate(research = grepl("research", Job_Description, ignore.case=TRUE)) %>%
    mutate(teamwork = grepl("teamwork", Job_Description, ignore.case=TRUE)) %>%
    mutate(integrity = grepl("integrity", Job_Description, ignore.case=TRUE)) %>%
    mutate(passionate= grepl("passionate", Job_Description, ignore.case=TRUE)) %>%
    select(Position, Company, workingremote, communication, collaborative, creative, critical, problemsolving, 
  activelearning, hypothesis, organized, judgement, selfstarter, interpersonalskills, detail_oriented, 
  visualization, leadership,presentation,passion,research,teamwork,integrity,passionate)
```



### Setup Soft skills for plotting


```{r}
softskills2 <- softskills %>% 
               select(-(1:2)) %>% 
               summarise_all(sum) %>% 
               gather(variable,value) %>% 
               arrange(desc(value))
```

### Visualized the most in-demand soft sKills:

```{r}
ggplot(softskills2,aes(x=reorder(variable, value), y=value)) + geom_bar(stat='identity',fill="orange",color="blue") + xlab('') + ylab('Frequency') + labs(title='Soft Skills') + coord_flip() + theme_minimal()
```

### Sentiment Analysis using sentimentr package

- sentiment of sentence within each line of job descriptions by job postings of individual companies


```{r}
scores1 <- sentiment_by(clean_allcsv2$Job_Description) %>%
  pull(ave_sentiment)
head(scores1)
#m_wide2 <- m_wide
#clean_allcsv2$sentiment_scores<- scores1
#clean_allcsv3 <- clean_allcsv2[,-c(4)] # deleting redundant column
sentiment_score_df <- clean_allcsv  %>% 
       select(Company, sentiment_scores)
head(sentiment_score_df)
```


###  Plotting the sentiments findings


```{r}
# Take a sample of 10 job listings of individual companies (top 10 vs. bottom 10 sentiment scores):
# First top 10 job listings 
df_top <- sentiment_score_df %>% 
  top_n(10)
# Bottom 10 job listings
df_bottom <- sentiment_score_df %>% 
  top_n(-10)

df_total <- left_join(df_top,df_bottom,by="Company")
df_total[,-c(3)] # deleting the extra column

```
  
```{r}
  
#Bar charts  to show 10 highest (Most Positive) sentiments Companies
top_sent <- ggplot(df_top , aes(x=Company,y=sentiment_scores,fill=Company)) + 
  geom_bar(color="black",stat="identity") +
  ggtitle("Top 10 Positive Sentiment Scores of Job Listings by Companies") +
  theme_excel() + 
  theme(axis.text = element_text(size = 10,angle =90, hjust = 1)) + 
  theme(legend.position="bottom")

top_sent
```

```{r}
#10 lowest (Least Positive) of Job Listings by Companies
lest_sent <- ggplot(df_bottom ,aes(x=Company,y=sentiment_scores,fill=Company)) + 
  geom_bar(color="black",stat="identity") + 
  ggtitle("Bottom 10 Lowest Sentiment Scores of Job Listings by Companies") +
  theme_excel() + 
  theme(axis.text = element_text(size = 10,angle =90, hjust = 1)) + 
  theme(legend.position="bottom")

lest_sent
```